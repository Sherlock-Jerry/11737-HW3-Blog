import{_ as s}from"./_plugin-vue_export-helper.cdc0426e.js";import{o as n,c as l,a as t,b as a,d as e}from"./app.6c75b31d.js";const i={},o=t('<p>With an understanding of the T5 model, now a question might arise in your mind as to when we have such a capable model, what is the need for a new model. The answer is that these language models are predominantly pre-trained on English-language text. Can you guess what population of the world does not speak English? 80 %. Yes, you read that right, there is still a large population in this world that does not speak English. The community has tried to largely address this problem by releasing additional models that are pretrained on a single non-English language but this may not be scalable as at a certain point it may not be feasible to cater a separate model for a separate language. This is where the multilingual setting comes in, where multilingual models are produced that are pre-trained on a mixture of multiple languages. And, the mT5 is one such multilingual model that aims to produce a multilingual model using the state of the art Text-to-Text Transformer architecture.</p><h2 id="the-data-that-powers-the-mt5-model" tabindex="-1"><a class="header-anchor" href="#the-data-that-powers-the-mt5-model" aria-hidden="true">#</a> The Data that powers the mT5 model</h2><p>In order to achieve the capability to work with multiple languages, naturally the mT5 model would need to be trained on a dataset corpus that contains text from different languages, 101 different languages to be precise. This multilingual dataset that mT5 is trained on is called the mC4 dataset. To explain further, the original T5 model is trained on the C4 dataset which is released by Common Crawl and the mC4 is a multilingual variant of this dataset. The need for this new dataset arises because the C4 dataset is English only in contrase to mC4 which consists of multiple languages. Common Crawl releases montly web scrapes which are used as data to train the model. Some interesting approaches to note that were adopted in order to accumulate this multilingual dataset are as follows:</p><ul><li>Line length filters are used in order to check whether there were at least 3 lines containing over 200 characters. The line length filter becomes necessary here because in the C4 dataset, the lines not ending with the English punctuation were removed but sentences in other languages may not follow this norm hence this new heuristic is defined.</li><li>How to identify if a piece of text belongs to a different language? The Compact Langauge Detector V3 (CLD3) is used in order to identify the language based on a piece of text. This tool emits a confidence score along with the detected language, and only those pieces of text were taken for which the confidence score was greater than 70%.</li><li>In order to collect this data involving over 100 languages, all 71 monthly scrapes released upto that point were used. In comparison for the original C4 dataset only a single scrape was sufficient. This fact shows the scarcity of data when a lot of languages need to be covered.</li><li>Text from various languages are grouped and languages for which 10,000 or more pages exist are kept in the final mC4 dataset.</li></ul><p>In this specific manner, the dataset was curated and it is important to note that the quality of the data determines the model performance and hence such care has to be taken in the collected data on which the model is to be trained on. Let&#39;s move on to understand how this data was used in order to train the mT5 model.</p><h2 id="specifics-of-mt5-pre-training" tabindex="-1"><a class="header-anchor" href="#specifics-of-mt5-pre-training" aria-hidden="true">#</a> Specifics of mT5 Pre-Training</h2><p>Now that we have understood the model architecture and the dataset that is prepared, let us further understand how these two components come together in order to create the mT5 model as we know. As discussed with respect to the dataset collected in order to train the mT5 model, it consists of 101 different languages and sampling data from each of these languages while pre-training becomes a major factor. This is an interesting problem and the way it has been tacked while pre-training the mT5 model is particularly interesting.</p><p>An approach called the zero-sum game is adopted. A simple way to think about the zero-sum game is it&#39;s like trying to balance two separate things: learning too much from languages with few resources or not learning enough from languages with plenty of resources. Essentially, if low-resource languages and sampled too often the model may tend to overfit and if the high resource languages are not trained on enough, the model will tend to overfit. In order to obtain the best of both worlds, the zero-sum game approach is adopted. The approach is not something that is completely new, but is adopted from prior research.</p>',8),r=a("p",null,[e("Diving into further technical aspects of this approach, low resource languages are boosted by sampling examples accoring to the probability equation represented by "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"p"),a("mo",{stretchy:"false"},"("),a("mi",null,"L"),a("mo",{stretchy:"false"},")"),a("mo",null,"\u221D"),a("mi",{mathvariant:"normal"},"\u2223"),a("mi",null,"L"),a("msup",null,[a("mi",{mathvariant:"normal"},"\u2223"),a("mi",null,"\u03B1")])]),a("annotation",{encoding:"application/x-tex"},"p(L) \\propto |L|^{\\alpha}")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord mathnormal"},"p"),a("span",{class:"mopen"},"("),a("span",{class:"mord mathnormal"},"L"),a("span",{class:"mclose"},")"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"\u221D"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord"},"\u2223"),a("span",{class:"mord mathnormal"},"L"),a("span",{class:"mord"},[a("span",{class:"mord"},"\u2223"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.6644em"}},[a("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mtight"},[a("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.0037em"}},"\u03B1")])])])])])])])])])])]),e(". Let us break this down in order to understand the nuances better.")],-1),h=a("ol",null,[a("li",null,[a("strong",null,"The Purpose"),a("ul",null,[a("li",null,"Extra attention needs to be given to low resource languages in comparison to the high resource languages."),a("li",null,[e("In the equation "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"p"),a("mo",{stretchy:"false"},"("),a("mi",null,"L"),a("mo",{stretchy:"false"},")"),a("mo",null,"\u221D"),a("mi",{mathvariant:"normal"},"\u2223"),a("mi",null,"L"),a("msup",null,[a("mi",{mathvariant:"normal"},"\u2223"),a("mi",null,"\u03B1")])]),a("annotation",{encoding:"application/x-tex"},"p(L) \\propto |L|^{\\alpha}")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord mathnormal"},"p"),a("span",{class:"mopen"},"("),a("span",{class:"mord mathnormal"},"L"),a("span",{class:"mclose"},")"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"\u221D"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord"},"\u2223"),a("span",{class:"mord mathnormal"},"L"),a("span",{class:"mord"},[a("span",{class:"mord"},"\u2223"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.6644em"}},[a("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mtight"},[a("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.0037em"}},"\u03B1")])])])])])])])])])])]),e(", "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"p"),a("mo",{stretchy:"false"},"("),a("mi",null,"L"),a("mo",{stretchy:"false"},")")]),a("annotation",{encoding:"application/x-tex"},"p(L)")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord mathnormal"},"p"),a("span",{class:"mopen"},"("),a("span",{class:"mord mathnormal"},"L"),a("span",{class:"mclose"},")")])])]),e(" is the probability of choosing text from a particular language "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"L")]),a("annotation",{encoding:"application/x-tex"},"L")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6833em"}}),a("span",{class:"mord mathnormal"},"L")])])]),e(" during training and here "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",{mathvariant:"normal"},"\u2223"),a("mi",null,"L"),a("mi",{mathvariant:"normal"},"\u2223")]),a("annotation",{encoding:"application/x-tex"},"|L|")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),a("span",{class:"mord"},"\u2223"),a("span",{class:"mord mathnormal"},"L"),a("span",{class:"mord"},"\u2223")])])]),e(" is the number of instances(sentences or pieces of text) belonging to the langauge "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"L")]),a("annotation",{encoding:"application/x-tex"},"L")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6833em"}}),a("span",{class:"mord mathnormal"},"L")])])]),e(".")])])]),a("li",null,[a("strong",null,[e("Understanding Hyperparameter "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"\u03B1")]),a("annotation",{encoding:"application/x-tex"},"\\alpha")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.4306em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"\u03B1")])])])]),a("ul",null,[a("li",null,[e("The hyperparameter "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"\u03B1")]),a("annotation",{encoding:"application/x-tex"},"\\alpha")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.4306em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"\u03B1")])])]),e(" is used to specify mathematically how much boost should low resource languages be given during the pre-training procedure. This hyperparameter can be adjusted and the most feasible number for this hyperparameter can be determined through experimentation.")]),a("li",null,[e("In a typical scenario, the value of "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"\u03B1")]),a("annotation",{encoding:"application/x-tex"},"\\alpha")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.4306em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"\u03B1")])])]),e(" is set to be less than 1 which can be anuything for example: 0.8, 0.6 or 0.3 etc.")])])]),a("li",null,[a("strong",null,[e("Determining the Value of "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"\u03B1")]),a("annotation",{encoding:"application/x-tex"},"\\alpha")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.4306em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"\u03B1")])])])]),a("ul",null,[a("li",null,[e("In order to find the optimal value of "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"\u03B1")]),a("annotation",{encoding:"application/x-tex"},"\\alpha")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.4306em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"\u03B1")])])]),e(" for the mT5 setting, the values used in prior research were experimented with such as 0.7, 0.3 and 0.2.")]),a("li",null,[e("After experimenting with these 3 values it was found that "),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"\u03B1"),a("mo",null,"="),a("mn",null,"0.3")]),a("annotation",{encoding:"application/x-tex"},"\\alpha = 0.3")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.4306em"}}),a("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"\u03B1"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},"="),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6444em"}}),a("span",{class:"mord"},"0.3")])])]),e(" worked most optimally, in the sense that this value provided a good balance between how the model performance varies for low-resouce and high-resource languages.")])])])],-1),m=t('<h2 id="comparison-with-other-models" tabindex="-1"><a class="header-anchor" href="#comparison-with-other-models" aria-hidden="true">#</a> Comparison with Other Models</h2><p>In order to better appreciate the capabilities that the mT5 model provides, it makes sense to also have a very brief overview of what the other models have accomplished in this field. Let us analyze a few models that have support for a few dozen languages in order to arrive at a fair comparison.</p><ol><li><strong>mBERT</strong>: Let us start with the mBERT model, which is multilingual variant of the well-known BERT model. The mBERT model follows the recipe of the BERT model closely in terms of the architecture and the objectives used. Once again, the dataset that mBERT is trained on differs from what BERT is trained. While BERT uses English Wikipedia and the Toronto Book Corpus, mBERT completely relies on text involving 104 languages from Wikipedia.</li><li><strong>XLM</strong>: The XLM is also based on BERT, but utilizes improved methods for pre-training multi-lingual language models through the inclusion of explicitly cross-lingual pre-training objectives.</li><li><strong>XLM-R</strong>: Based on the name XLM-R, you would think this model is related to the XLM model, and in fact you are right. The XLM-R is an improved version of the XLM and is based on the RoBERTa model. It is trained on data in 100 languages from Common Crawl and involves a cross-lingual masked language model objective.</li><li><strong>mBART</strong>: The mBART is based on the BART model and is a mulilingual encoder-decoder architecture. The mBART training involves a combination of span masking and sentence shuffling objectives. When it comes to the data that mBART was trained on, the dataset involves a subset of 25 languages picked up from the same data as that of XLM-R.</li><li><strong>MARGE</strong>: MARGE is a multilingual encoder decoder model and is trained to reconstruct a document in one language by using data retreived from documents in other languages. The dataset involves text from 26 differnt langauges and is from Wikipedia and CC-News.</li></ol>',3),c=[o,r,h,m];function p(d,u){return n(),l("div",null,c)}const f=s(i,[["render",p],["__file","Backup.html.vue"]]);export{f as default};
