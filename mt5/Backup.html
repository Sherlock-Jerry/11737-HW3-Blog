<!DOCTYPE html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.51" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://lileicc.github.io/blog/mt5/Backup.html"><meta property="og:site_name" content="MLNLP Blog"><meta property="og:type" content="article"><meta property="og:updated_time" content="2023-11-10T20:39:08.000Z"><meta property="og:locale" content="en-US"><meta property="article:modified_time" content="2023-11-10T20:39:08.000Z"><title>MLNLP Blog</title><meta name="description" content="A Blog for Machine Learning, Natural Language Processing, and Data Mining">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d2025;
      }

      html,
      body {
        background-color: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="stylesheet" href="/blog/assets/style.758fbe27.css">
    <link rel="modulepreload" href="/blog/assets/app.d7cc616f.js"><link rel="modulepreload" href="/blog/assets/Backup.html.e45815c8.js"><link rel="modulepreload" href="/blog/assets/_plugin-vue_export-helper.cdc0426e.js"><link rel="modulepreload" href="/blog/assets/Backup.html.d0e2f1fa.js"><link rel="prefetch" href="/blog/assets/index.html.48d72793.js"><link rel="prefetch" href="/blog/assets/index.html.b50b4d64.js"><link rel="prefetch" href="/blog/assets/404.html.144ea56c.js"><link rel="prefetch" href="/blog/assets/index.html.3a32b4c3.js"><link rel="prefetch" href="/blog/assets/index.html.3eaba85c.js"><link rel="prefetch" href="/blog/assets/index.html.9c8e527a.js"><link rel="prefetch" href="/blog/assets/index.html.949c6932.js"><link rel="prefetch" href="/blog/assets/index.html.9369be8b.js"><link rel="prefetch" href="/blog/assets/index.html.5ecb22f0.js"><link rel="prefetch" href="/blog/assets/index.html.0a31196e.js"><link rel="prefetch" href="/blog/assets/index.html.e354f66f.js"><link rel="prefetch" href="/blog/assets/index.html.1365a3e0.js"><link rel="prefetch" href="/blog/assets/404.html.0e23319c.js"><link rel="prefetch" href="/blog/assets/index.html.982b11f0.js"><link rel="prefetch" href="/blog/assets/index.html.86009c85.js"><link rel="prefetch" href="/blog/assets/index.html.58acb1fb.js"><link rel="prefetch" href="/blog/assets/index.html.bc300717.js"><link rel="prefetch" href="/blog/assets/index.html.949009b4.js"><link rel="prefetch" href="/blog/assets/index.html.cdb327b3.js"><link rel="prefetch" href="/blog/assets/index.html.a4abcc21.js"><link rel="prefetch" href="/blog/assets/giscus.15440425.js"><link rel="prefetch" href="/blog/assets/highlight.esm.d982e650.js"><link rel="prefetch" href="/blog/assets/markdown.esm.832a189d.js"><link rel="prefetch" href="/blog/assets/math.esm.a3f84b6f.js"><link rel="prefetch" href="/blog/assets/notes.esm.3c361cb7.js"><link rel="prefetch" href="/blog/assets/reveal.esm.b96f05d8.js"><link rel="prefetch" href="/blog/assets/search.esm.80da4a02.js"><link rel="prefetch" href="/blog/assets/zoom.esm.8514a202.js"><link rel="prefetch" href="/blog/assets/photoswipe.esm.382b1873.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">Skip to content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><!--[--><header class="navbar"><div class="navbar-left"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><a href="/blog/" class="brand"><img class="logo" src="/blog/logo.svg" alt="MLNLP Blog"><!----><span class="site-name hide-in-pad">MLNLP Blog</span></a><!----></div><div class="navbar-center"><!----><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/blog/" class="nav-link" aria-label="Blog Home"><span class="icon iconfont icon-home"></span>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/category/" class="nav-link" aria-label="Category"><span class="icon iconfont icon-categoryselected"></span>Category<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/tag/" class="nav-link" aria-label="Tags"><span class="icon iconfont icon-tag"></span>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/timeline/" class="nav-link" aria-label="Timeline"><span class="icon iconfont icon-time"></span>Timeline<!----></a></div></nav><!----></div><div class="navbar-right"><!----><!----><div class="nav-item"><a class="repo-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!----><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow left"></span></div><aside class="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!----></h1><div class="page-info"><span class="author-info" aria-label="Author🖊" data-balloon-pos="down" localizeddate="November 10, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="author-item" href="https://www.cs.ucsb.edu/~leili" target="_blank" rel="noopener noreferrer">Lei Li</a></span><span property="author" content="Lei Li"></span></span><!----><span class="date-info" aria-label="Writing Date📅" data-balloon-pos="down" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span>November 10, 2023</span><meta property="datePublished" content="2023-11-10T01:01:15.000Z"></span><!----><!----><span class="reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down" localizeddate="November 10, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 5 min</span><meta property="timeRequired" content="PT5M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">On This Page</div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/mt5/Backup.html#the-data-that-powers-the-mt5-model" class="router-link-active router-link-exact-active toc-link level2">The Data that powers the mT5 model</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/mt5/Backup.html#specifics-of-mt5-pre-training" class="router-link-active router-link-exact-active toc-link level2">Specifics of mT5 Pre-Training</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/mt5/Backup.html#comparison-with-other-models" class="router-link-active router-link-exact-active toc-link level2">Comparison with Other Models</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><p>With an understanding of the T5 model, now a question might arise in your mind as to when we have such a capable model, what is the need for a new model. The answer is that these language models are predominantly pre-trained on English-language text. Can you guess what population of the world does not speak English? 80 %. Yes, you read that right, there is still a large population in this world that does not speak English. The community has tried to largely address this problem by releasing additional models that are pretrained on a single non-English language but this may not be scalable as at a certain point it may not be feasible to cater a separate model for a separate language. This is where the multilingual setting comes in, where multilingual models are produced that are pre-trained on a mixture of multiple languages. And, the mT5 is one such multilingual model that aims to produce a multilingual model using the state of the art Text-to-Text Transformer architecture.</p><h2 id="the-data-that-powers-the-mt5-model" tabindex="-1"><a class="header-anchor" href="#the-data-that-powers-the-mt5-model" aria-hidden="true">#</a> The Data that powers the mT5 model</h2><p>In order to achieve the capability to work with multiple languages, naturally the mT5 model would need to be trained on a dataset corpus that contains text from different languages, 101 different languages to be precise. This multilingual dataset that mT5 is trained on is called the mC4 dataset. To explain further, the original T5 model is trained on the C4 dataset which is released by Common Crawl and the mC4 is a multilingual variant of this dataset. The need for this new dataset arises because the C4 dataset is English only in contrase to mC4 which consists of multiple languages. Common Crawl releases montly web scrapes which are used as data to train the model. Some interesting approaches to note that were adopted in order to accumulate this multilingual dataset are as follows:</p><ul><li>Line length filters are used in order to check whether there were at least 3 lines containing over 200 characters. The line length filter becomes necessary here because in the C4 dataset, the lines not ending with the English punctuation were removed but sentences in other languages may not follow this norm hence this new heuristic is defined.</li><li>How to identify if a piece of text belongs to a different language? The Compact Langauge Detector V3 (CLD3) is used in order to identify the language based on a piece of text. This tool emits a confidence score along with the detected language, and only those pieces of text were taken for which the confidence score was greater than 70%.</li><li>In order to collect this data involving over 100 languages, all 71 monthly scrapes released upto that point were used. In comparison for the original C4 dataset only a single scrape was sufficient. This fact shows the scarcity of data when a lot of languages need to be covered.</li><li>Text from various languages are grouped and languages for which 10,000 or more pages exist are kept in the final mC4 dataset.</li></ul><p>In this specific manner, the dataset was curated and it is important to note that the quality of the data determines the model performance and hence such care has to be taken in the collected data on which the model is to be trained on. Let&#39;s move on to understand how this data was used in order to train the mT5 model.</p><h2 id="specifics-of-mt5-pre-training" tabindex="-1"><a class="header-anchor" href="#specifics-of-mt5-pre-training" aria-hidden="true">#</a> Specifics of mT5 Pre-Training</h2><p>Now that we have understood the model architecture and the dataset that is prepared, let us further understand how these two components come together in order to create the mT5 model as we know. As discussed with respect to the dataset collected in order to train the mT5 model, it consists of 101 different languages and sampling data from each of these languages while pre-training becomes a major factor. This is an interesting problem and the way it has been tacked while pre-training the mT5 model is particularly interesting.</p><p>An approach called the zero-sum game is adopted. A simple way to think about the zero-sum game is it&#39;s like trying to balance two separate things: learning too much from languages with few resources or not learning enough from languages with plenty of resources. Essentially, if low-resource languages and sampled too often the model may tend to overfit and if the high resource languages are not trained on enough, the model will tend to overfit. In order to obtain the best of both worlds, the zero-sum game approach is adopted. The approach is not something that is completely new, but is adopted from prior research.</p><p>Diving into further technical aspects of this approach, low resource languages are boosted by sampling examples accoring to the probability equation represented by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo><mo>∝</mo><mi mathvariant="normal">∣</mi><mi>L</mi><msup><mi mathvariant="normal">∣</mi><mi>α</mi></msup></mrow><annotation encoding="application/x-tex">p(L) \propto |L|^{\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span></span></span></span></span></span></span></span>. Let us break this down in order to understand the nuances better.</p><ol><li><strong>The Purpose</strong><ul><li>Extra attention needs to be given to low resource languages in comparison to the high resource languages.</li><li>In the equation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo><mo>∝</mo><mi mathvariant="normal">∣</mi><mi>L</mi><msup><mi mathvariant="normal">∣</mi><mi>α</mi></msup></mrow><annotation encoding="application/x-tex">p(L) \propto |L|^{\alpha}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mclose">)</span></span></span></span> is the probability of choosing text from a particular language <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span> during training and here <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>L</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|L|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">L</span><span class="mord">∣</span></span></span></span> is the number of instances(sentences or pieces of text) belonging to the langauge <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span>.</li></ul></li><li><strong>Understanding Hyperparameter <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></strong><ul><li>The hyperparameter <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is used to specify mathematically how much boost should low resource languages be given during the pre-training procedure. This hyperparameter can be adjusted and the most feasible number for this hyperparameter can be determined through experimentation.</li><li>In a typical scenario, the value of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is set to be less than 1 which can be anuything for example: 0.8, 0.6 or 0.3 etc.</li></ul></li><li><strong>Determining the Value of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></strong><ul><li>In order to find the optimal value of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> for the mT5 setting, the values used in prior research were experimented with such as 0.7, 0.3 and 0.2.</li><li>After experimenting with these 3 values it was found that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.3</span></span></span></span> worked most optimally, in the sense that this value provided a good balance between how the model performance varies for low-resouce and high-resource languages.</li></ul></li></ol><h2 id="comparison-with-other-models" tabindex="-1"><a class="header-anchor" href="#comparison-with-other-models" aria-hidden="true">#</a> Comparison with Other Models</h2><p>In order to better appreciate the capabilities that the mT5 model provides, it makes sense to also have a very brief overview of what the other models have accomplished in this field. Let us analyze a few models that have support for a few dozen languages in order to arrive at a fair comparison.</p><ol><li><strong>mBERT</strong>: Let us start with the mBERT model, which is multilingual variant of the well-known BERT model. The mBERT model follows the recipe of the BERT model closely in terms of the architecture and the objectives used. Once again, the dataset that mBERT is trained on differs from what BERT is trained. While BERT uses English Wikipedia and the Toronto Book Corpus, mBERT completely relies on text involving 104 languages from Wikipedia.</li><li><strong>XLM</strong>: The XLM is also based on BERT, but utilizes improved methods for pre-training multi-lingual language models through the inclusion of explicitly cross-lingual pre-training objectives.</li><li><strong>XLM-R</strong>: Based on the name XLM-R, you would think this model is related to the XLM model, and in fact you are right. The XLM-R is an improved version of the XLM and is based on the RoBERTa model. It is trained on data in 100 languages from Common Crawl and involves a cross-lingual masked language model objective.</li><li><strong>mBART</strong>: The mBART is based on the BART model and is a mulilingual encoder-decoder architecture. The mBART training involves a combination of span masking and sentence shuffling objectives. When it comes to the data that mBART was trained on, the dataset involves a subset of 25 languages picked up from the same data as that of XLM-R.</li><li><strong>MARGE</strong>: MARGE is a multilingual encoder decoder model and is trained to reconstruct a document in one language by using data retreived from documents in other languages. The dataset involves text from 26 differnt langauges and is from Wikipedia and CC-News.</li></ol></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/lileicc/blog/edit/main/mt5/Backup.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item update-time"><span class="label">Last update: </span><span class="info">11/10/2023, 8:39:08 PM</span></div><div class="meta-item contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: akhil.e2k@gmail.com">Akhil E</span><!--]--><!--]--></div></footer><!----><div class="giscus-wrapper input-top" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer">Li Lab</div><div class="copyright">Copyright © 2023 Lei Li</div></footer><!--]--></div><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app.d7cc616f.js" defer></script>
  </body>
</html>
