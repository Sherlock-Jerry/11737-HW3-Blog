With an understanding of the T5 model, now a question might arise in your mind as to when we have such a capable model, what is the need for a new model. The answer is that these language models are predominantly pre-trained on English-language text. Can you guess what population of the world does not speak English? 80 %. Yes, you read that right, there is still a large population in this world that does not speak English. The community has tried to largely address this problem by releasing additional models that are pretrained on a single non-English language but this may not be scalable as at a certain point it may not be feasible to cater a separate model for a separate language. This is where the multilingual setting comes in, where multilingual models are produced that are pre-trained on a mixture of multiple languages. And, the mT5 is one such multilingual model that aims to produce a multilingual model using the state of the art Text-to-Text Transformer architecture. 