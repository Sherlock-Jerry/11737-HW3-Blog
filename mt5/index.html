<!DOCTYPE html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.51" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://sherlock-jerry.github.io/mt5/"><meta property="og:site_name" content="mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer | 11737-HW3-Blog"><meta property="og:title" content="EXPLAINED - mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer"><meta property="og:type" content="article"><meta property="og:image" content="https://sherlock-jerry.github.io/"><meta property="og:updated_time" content="2023-11-16T21:41:14.000Z"><meta property="og:locale" content="en-US"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="EXPLAINED - mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer"><meta property="article:modified_time" content="2023-11-16T21:41:14.000Z"><title>EXPLAINED - mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer | mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer | 11737-HW3-Blog</title><meta name="description" content="A Blog for Machine Learning, Natural Language Processing, and Data Mining">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d2025;
      }

      html,
      body {
        background-color: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="stylesheet" href="/assets/style.c54272bd.css">
    <link rel="modulepreload" href="/assets/app.5ae08124.js"><link rel="modulepreload" href="/assets/index.html.3de74b30.js"><link rel="modulepreload" href="/assets/_plugin-vue_export-helper.cdc0426e.js"><link rel="modulepreload" href="/assets/index.html.c1c96512.js"><link rel="prefetch" href="/assets/index.html.3ccab1a5.js"><link rel="prefetch" href="/assets/Backup.html.842badcb.js"><link rel="prefetch" href="/assets/404.html.30c854e9.js"><link rel="prefetch" href="/assets/index.html.00b07dc8.js"><link rel="prefetch" href="/assets/index.html.cb286504.js"><link rel="prefetch" href="/assets/index.html.57855ad9.js"><link rel="prefetch" href="/assets/index.html.3f88b7c5.js"><link rel="prefetch" href="/assets/index.html.3b69684f.js"><link rel="prefetch" href="/assets/index.html.1d42aa36.js"><link rel="prefetch" href="/assets/index.html.e063a744.js"><link rel="prefetch" href="/assets/index.html.8b8950cd.js"><link rel="prefetch" href="/assets/Backup.html.cb4063de.js"><link rel="prefetch" href="/assets/404.html.c2fa59e4.js"><link rel="prefetch" href="/assets/index.html.c695d33a.js"><link rel="prefetch" href="/assets/index.html.e4f346fb.js"><link rel="prefetch" href="/assets/index.html.f9a19827.js"><link rel="prefetch" href="/assets/index.html.e42e7774.js"><link rel="prefetch" href="/assets/index.html.8d41cce2.js"><link rel="prefetch" href="/assets/index.html.901e3667.js"><link rel="prefetch" href="/assets/index.html.4b571e5a.js"><link rel="prefetch" href="/assets/giscus.15440425.js"><link rel="prefetch" href="/assets/highlight.esm.d982e650.js"><link rel="prefetch" href="/assets/markdown.esm.832a189d.js"><link rel="prefetch" href="/assets/math.esm.a3f84b6f.js"><link rel="prefetch" href="/assets/notes.esm.3c361cb7.js"><link rel="prefetch" href="/assets/reveal.esm.b96f05d8.js"><link rel="prefetch" href="/assets/search.esm.80da4a02.js"><link rel="prefetch" href="/assets/zoom.esm.8514a202.js"><link rel="prefetch" href="/assets/photoswipe.esm.382b1873.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">Skip to content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><!--[--><header class="navbar"><div class="navbar-left"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><a href="/" class="brand"><img class="logo" src="/logo.svg" alt="mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer | 11737-HW3-Blog"><!----><span class="site-name hide-in-pad">mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer | 11737-HW3-Blog</span></a><!----></div><div class="navbar-center"><!----><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/" class="nav-link" aria-label="11737, MNLP - HW3 Blog"><!---->11737, MNLP - HW3 Blog<!----></a></div><div class="nav-item hide-in-mobile"><a href="/category/" class="nav-link" aria-label="Category"><span class="icon iconfont icon-categoryselected"></span>Category<!----></a></div><div class="nav-item hide-in-mobile"><a href="/tag/" class="nav-link" aria-label="Tags"><span class="icon iconfont icon-tag"></span>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a href="/timeline/" class="nav-link" aria-label="Timeline"><span class="icon iconfont icon-time"></span>Timeline<!----></a></div></nav><!----></div><div class="navbar-right"><!----><!----><div class="nav-item"><a class="repo-link" href="https://github.com/sherlock-jerry/11737-HW3-Blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!----><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow left"></span></div><aside class="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->EXPLAINED - mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer</h1><div class="page-info"><span class="author-info" aria-label="Author🖊" data-balloon-pos="down" localizeddate="November 10, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="author-item">Lei Li</span></span><span property="author" content="Lei Li"></span></span><!----><span class="date-info" aria-label="Writing Date📅" data-balloon-pos="down" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span>November 10, 2023</span><meta property="datePublished" content="2023-11-10T01:00:55.000Z"></span><!----><!----><span class="reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down" localizeddate="November 10, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 14 min</span><meta property="timeRequired" content="PT14M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">On This Page</div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#authors" class="router-link-active router-link-exact-active toc-link level2">Authors</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#introduction" class="router-link-active router-link-exact-active toc-link level2">Introduction</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#background" class="router-link-active router-link-exact-active toc-link level2">Background</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#introducing-the-mt5-model" class="router-link-active router-link-exact-active toc-link level2">Introducing the mT5 Model</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#the-data-that-powers-the-mt5-model" class="router-link-active router-link-exact-active toc-link level2">The Data that powers the mT5 model</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#dive-into-mt5-architecture" class="router-link-active router-link-exact-active toc-link level2">Dive into mT5 Architecture</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#attention-mask-patterns" class="router-link-active router-link-exact-active toc-link level3">Attention Mask Patterns</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#model-architecture-candidates" class="router-link-active router-link-exact-active toc-link level3">Model Architecture Candidates</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#specifics-of-mt5-pre-training" class="router-link-active router-link-exact-active toc-link level2">Specifics of mT5 Pre-Training</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#experiments" class="router-link-active router-link-exact-active toc-link level2">Experiments</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#comparison-with-other-models" class="router-link-active router-link-exact-active toc-link level2">Comparison with Other Models</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#results" class="router-link-active router-link-exact-active toc-link level2">Results</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#zero-shot-generation-setting" class="router-link-active router-link-exact-active toc-link level2">Zero-Shot Generation Setting</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#capabilities-through-examples" class="router-link-active router-link-exact-active toc-link level2">Capabilities through Examples</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#wrapping-it-up" class="router-link-active router-link-exact-active toc-link level2">Wrapping it up</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/mt5/#references" class="router-link-active router-link-exact-active toc-link level2">References</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><h1 id="explained-mt5-a-massively-multilingual-pre-trained-text-to-text-transformer" tabindex="-1"><a class="header-anchor" href="#explained-mt5-a-massively-multilingual-pre-trained-text-to-text-transformer" aria-hidden="true">#</a> EXPLAINED - mT5: A Massively Multilingual Pre-Trained Text-To-Text Transformer</h1><h2 id="authors" tabindex="-1"><a class="header-anchor" href="#authors" aria-hidden="true">#</a> Authors</h2><ol><li>Akhil Eppa - <a href="aeppa@cs.cmu.edu">aeppa@cs.cmu.edu</a></li><li>R Raghav - <a href="rraghavr@cs.cmu.edu">rraghavr@cs.cmu.edu</a></li></ol><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p>In this blog post, we&#39;ll delve into the intricacies of the mT5 model, a multilingual iteration of the T5 model. This variant has undergone training on a new dataset, showcasing improved performance across a diverse range of multilingual benchmarks. Notably, the mT5 model exhibits enhancements in tasks associated with the multilingual setting, all while leveraging the robust architecture of the &quot;Text-to-Text Transformer&quot; (T5) model.</p><p>By exploring this blog, you&#39;ll gain a comprehensive understanding of the mT5 model, unraveling its background and the journey that led to its development. We aim to shed light on the nuances behind the mT5 model, highlighting its features and the impact it has in the realm of multilingual natural language processing.</p><p align="center"><img src="/assets/Header_Image.231d52f7.png" alt="Header Image" width="75%"></p><h2 id="background" tabindex="-1"><a class="header-anchor" href="#background" aria-hidden="true">#</a> Background</h2><p>To gain a better understanding of the mT5 model, let&#39;s start by exploring an overview of the T5 model and delving into what it means to accomplish a multilingual task. The T5 model, short for Text-to-Text Transformer, is a cutting-edge model designed for processing natural language. Its strength lies in the transformer architecture, an advanced neural network that excels in capturing intricate relations within text sequences, making it particularly adept at understanding and generating textual content.</p><p>In simpler terms, think of the T5 model as a super-smart computer program capable of understanding and generating human-like text. To acquire this ability, the T5 model undergoes training on extensive datasets through an iterative learning process. This process refines its understanding of language structure and semantic relationships, turning it into a proficient text generator.</p><p>What makes the T5 model even more exciting is its versatility. It&#39;s not confined to a specific task but can accomplish a variety of them, including translating languages, summarizing text, answering questions, and more. You might wonder how a single model can perform such diverse tasks. This is achieved by adding task-specific prefixes to the input sequence and pre-training the model to produce prefix-specific outputs.The T5 model undergoes both unsupervised and supervised pre-training. In the unsupervised setting, spans are masked with a special token, and the model predicts the masked sequence. In the supervised setting, the input consists of the task name followed by the required input text, and the expected target text is provided as the corresponding output. This is just a high-level overview of the T5 model to set the stage for what comes next and bridge the gap in understanding the mT5 model.</p><p><img src="/assets/t5_example.2067c4f9.gif" alt="T5 Illustration"></p><p><em>Illustration of the T5 Model. <a href="https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html" target="_blank" rel="noopener noreferrer">Image Credits<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></em></p><p>Having explored the T5 model, you might be pondering why we need a new model when we already have such a capable one. The answer lies in the fact that these language models are primarily pre-trained on English-language text. Now, consider this: what percentage of the world&#39;s population doesn&#39;t speak English? A whopping 80%. Yes, you read that right. There is still a significant portion of our global community that doesn&#39;t communicate in English.</p><p>Efforts have been made to address this language gap by releasing additional models pre-trained on a single non-English language. However, this approach may not be scalable. Imagine trying to create a separate model for every language—it becomes impractical at a certain point. This is where the concept of a multilingual setting becomes crucial. In this setting, models are developed to be pre-trained on a mix of multiple languages, addressing the challenge of linguistic diversity more effectively.</p><p>Enter the mT5, a multilingual model aiming to bridge language gaps by utilizing the state-of-the-art Text-to-Text Transformer architecture. It&#39;s designed to be a versatile solution, capable of understanding and generating text in multiple languages. The mT5 model represents a step forward in making advanced language processing accessible to a more diverse global audience.</p><h2 id="introducing-the-mt5-model" tabindex="-1"><a class="header-anchor" href="#introducing-the-mt5-model" aria-hidden="true">#</a> Introducing the mT5 Model</h2><p>The Multilingual T5, or mT5 for short, is, as the name suggests, a multilingual variant of the popular T5 model. To achieve its multilingual capability, the model underwent pretraining on a dataset covering a staggering 101 different languages. The model itself boasts an impressive parameter range, spanning between 300 million and 13 billion. This encompasses five different variants, starting with the smaller variant containing 300 million parameters, and scaling up to the XXL variant with a substantial 13 billion parameters.</p><p>Through its extensive pretraining across a diverse array of languages, mT5 showcases its remarkable capacity to generalize across linguistic variations. This capability contributes significantly to the improvement of models in low-resource language domains. The architecture of mT5 facilitates effective information transfer and knowledge sharing among languages, highlighting its adaptability and utility in scenarios where linguistic resources are limited.</p><p>In the upcoming sections, we will delve into the preparation of the dataset on which the model was trained and explore the nuances of the training strategy. Specifically, we&#39;ll discuss how the training strategy, initially designed for the T5 model, was fine-tuned to cater to the unique requirements of the mT5 model. Additionally, we will examine the performance demonstrated by the mT5 model and engage in a discussion on its capabilities.</p><h2 id="the-data-that-powers-the-mt5-model" tabindex="-1"><a class="header-anchor" href="#the-data-that-powers-the-mt5-model" aria-hidden="true">#</a> The Data that powers the mT5 model</h2><p>To enable the mT5 model to proficiently handle multiple languages, it undergoes training on a diverse dataset known as the mC4 dataset, encompassing text from a comprehensive set of 101 languages. Unlike the original T5 model, which relied on the English-only C4 dataset from Common Crawl, the mC4 dataset is a multilingual adaptation, recognizing the necessity for linguistic diversity.</p><p>The creation of this multilingual dataset involves meticulous approaches:</p><ul><li>Line length filters are implemented to ensure that at least three lines, each containing over 200 characters, are included. This filter addresses the variance observed in sentences of languages other than English within the C4 dataset.</li><li>Identifying different languages within the dataset is accomplished using the Compact Language Detector V3 (CLD3). This tool assigns a confidence score along with the detected language, and only text pieces with a confidence score surpassing 70% are retained.</li><li>Over 100 languages are encompassed by aggregating data from 71 monthly web scrapes released by Common Crawl. This stands in stark contrast to the original C4 dataset, where a single scrape sufficed, underscoring the challenge of obtaining comprehensive data when dealing with a multitude of languages.</li><li>The final mC4 dataset selects text from various languages, retaining those with 10,000 or more pages, emphasizing a threshold for inclusion based on language volume.</li></ul><p>It&#39;s crucial to recognize that the quality of the data profoundly influences model performance. Consequently, meticulous attention is dedicated to curating the dataset that serves as the foundation for training the mT5 model. In the subsequent sections, we&#39;ll delve into how this curated data is leveraged to train and optimize the capabilities of the mT5 model.</p><h2 id="dive-into-mt5-architecture" tabindex="-1"><a class="header-anchor" href="#dive-into-mt5-architecture" aria-hidden="true">#</a> Dive into mT5 Architecture</h2><p>The architecture of mT5 closely followed that of the T5 model. In fact it is based on an improved version of the T5 model called the T5v1.1 which is a slight improvement of the T5 model with some minor architectural tweaks. Essentially the mT5 architecture can be understood by studying the architecture of the T5 model.</p><h3 id="attention-mask-patterns" tabindex="-1"><a class="header-anchor" href="#attention-mask-patterns" aria-hidden="true">#</a> Attention Mask Patterns</h3><p>It is important that we understand about the different attention mask patterns, before we proceed to understand T5 model architecture</p><p><img src="/assets/attention_patterns.f5e0112c.png" alt="Attention Patterns"></p><p>The attention masks control which parts of the input elements the self-attention mechanism can focus on. The matrices visually represent these patterns.</p><p>Left - The fully-visible mask permits the self-attention mechanism to consider the entire input for every output step.</p><p>Middle - In contrast, the causal mask restricts the output at step &#39;i&#39; from relying on input elements occurring after step &#39;i&#39;, preventing dependence on future elements.</p><p>Right - The causal mask with a prefix allows a hybrid approach by enabling unrestricted attention for a section of the input sequence while enforcing causal masking for the rest, maintaining the restriction on future elements for specific parts of the sequence.</p><h3 id="model-architecture-candidates" tabindex="-1"><a class="header-anchor" href="#model-architecture-candidates" aria-hidden="true">#</a> Model Architecture Candidates</h3><p>When assessing various architectures suitable for language models, the authors of T5 primarily considered three types:</p><ol><li><p>Encoder-Decoder: This conventional structure involves an encoder-decoder setup, utilizing full visibility in the encoder and encoder-decoder attention. Causal masking is implemented in the decoder to prevent future output dependencies, ensuring predictions are made without accessing information from future positions.</p></li><li><p>Language Model (LM): In this configuration, a single stack of Transformer layers forms the model. It receives the concatenation of input and target sequences while employing a causal mask consistently. As with typical language models, the output is restricted to attending only to past input or output elements.</p></li><li><p>Prefix LM: The Prefix LM is an extension of the language model, incorporating the allowance for fully-visible masking over a segment of the input. This variant operates similarly to an LM, with the distinction that the output attends to a specific portion of the input—typically containing task-specific information (like translating English to German)—present in the prefix.</p></li></ol><p>The three model architecture types are depicted in the below figure -</p><p align="center"><img src="/assets/t5_transformer_archi_variant.bdb30a9b.png" alt="Transformer Architecture Schematics"></p><p>The authors found that the Transformer (Encoder-Decoder) based architecture exhibits superior performance compared to alternative architectures, as shown by the results below - <img src="/assets/t5_arch_comp_results.7ac541f1.png" alt="Architecture Comparison Results"></p><h2 id="specifics-of-mt5-pre-training" tabindex="-1"><a class="header-anchor" href="#specifics-of-mt5-pre-training" aria-hidden="true">#</a> Specifics of mT5 Pre-Training</h2><p>Now that we&#39;ve grasped the intricacies of the model architecture and the intricately prepared dataset, let&#39;s delve into how these vital components synergize to craft the mT5 model. Examining the dataset crucial for training mT5, which spans 101 diverse languages, the process of sampling data during pre-training takes center stage. The methodology employed in pre-training mT5 to tackle this linguistic diversity is intriguing and noteworthy.</p><p>Enter the zero-sum game approach, a key player in the pre-training of mT5. Think of it as a delicate balancing act – akin to walking a tightrope between learning too much from languages with limited resources and not learning enough from languages with abundant resources. Picture this: if we focus too much on languages with scarce resources, the model might become too specialized (overfitting), and if we neglect languages with plenty of resources, the model might not generalize well (underfitting). Striking the right balance is where the zero-sum game comes into play, a strategy not entirely novel but drawn from previous research, ensuring mT5 learns effectively from a diverse linguistic landscape.</p><p>Diving into further technical aspects of this approach, low resource languages are boosted by sampling examples accoring to the probability equation represented by p(L) α (|L| ^ α). Let us break this down in order to understand the nuances better.</p><ol><li><strong>The Purpose: Focusing on Linguistic Diversity</strong><ul><li>It&#39;s crucial to prioritize low-resource languages, ensuring they receive more attention than their high-resource counterparts.</li><li>In the equation p(L) α (|L| ^ α), p(L) signifies the likelihood of selecting text from a specific language L during training, while |L| represents the count of instances (sentences or text pieces) in language L.</li></ul></li><li><strong>Understanding Hyperparameter α: A Tuning Knob</strong><ul><li>The hyperparameter α quantifies the boost given to low-resource languages during pre-training. Adjusting α allows for fine-tuning this boost, and the value of α is determined through experimentation.</li><li>Typically set to a value less than 1 (e.g., 0.8, 0.6, or 0.3), α plays a pivotal role in shaping the model&#39;s attention distribution.</li></ul></li><li><strong>Determining the Optimal α</strong><ul><li>To identify the most effective α for the mT5 setting, experiments were conducted with values from prior research: 0.7, 0.3 and 0.2.</li><li>After experimenting with these 3 values it was found that α = 0.3 worked most optimally, in the sense that it struck a balance, showcasing optimal model performance across both low-resource and high-resource languages.</li></ul></li></ol><p>In essence, this dynamic approach adjusts the attention given to different languages during the pre-training process, ensuring the model learns effectively from a diverse linguistic landscape. As mentioned earlier, the α parameter becomes the key to striking the right balance, making the mT5 model versatile and robust in handling various language scenarios, which we will further discuss in subsequent sections.</p><h2 id="experiments" tabindex="-1"><a class="header-anchor" href="#experiments" aria-hidden="true">#</a> Experiments</h2><p>The evaluation of mT5 involved testing its performance across six tasks sourced from the XTREME multilingual benchmark. These tasks covered multilingual challenges such as entailment, reading comprehension, named entity recognition, and paraphrase identification in various languages. The approach included transforming tasks into a text-to-text format, generating label text, entity tags, or direct answers. Three task variants were explored: &quot;zero-shot&quot; fine-tuning on English data only, &quot;translate-train&quot; with machine translations into target languages, and &quot;in-language multitask&quot; with training on available gold data in all target languages.</p><p>In accordance with the original T5 framework, the study explores five model sizes: Small (around 300 million parameters), Base (580 million), Large (1.2 billion), XL (3.7 billion), and XXL (13 billion). The increase in parameter count in mT5 compared to T5 models is attributed to a larger vocabulary. Notably, mT5, as an encoder-decoder model, approximately doubles the parameters of similarly sized encoder-only models like XLM-R. For instance, while the XLM-R &quot;Large&quot; variant has 550 million parameters, mT5-Large has roughly 1 billion. However, the computational expense for text classification remains similar. In both models, a sequence of length &#39;T&#39; is processed by an encoder of nearly equal size. While an encoder-only model like XLM-R deals with one additional &quot;CLS&quot; token for classification, mT5&#39;s decoder usually generates two extra tokens: the class label and an end-of-sequence token. Despite this, the computational cost for classification with mT5 usually involves processing &#39;T + 2&#39; tokens, compared to &#39;T + 1&#39; for an encoder-only model. Notably, the encoder-decoder architecture offers added versatility for generative tasks like abstractive summarization or dialogues.</p><p>Their pre-training methodology involved training mT5 model variants for a million steps, each on batches comprising 1024 sequences of length 1024, totaling roughly 1 trillion input tokens. This pre-training duration matches that of T5 and is approximately one-sixth of XLM-R&#39;s pre-training scale. They adopted T5&#39;s inverse square-root learning rate schedule during pre-training. Aligning with the T5v1.1 approach, dropout was not applied during pre-training. Their self-supervised objective mirrored T5, involving masking 15% of tokens with an average noise span length of 3. For fine-tuning, a constant learning rate of 0.001 and a 0.1 dropout rate were employed across all tasks. While most tasks used a batch size of 2^17, in a few cases, they scaled this up to 2^20 based on validation set performance.</p><h2 id="comparison-with-other-models" tabindex="-1"><a class="header-anchor" href="#comparison-with-other-models" aria-hidden="true">#</a> Comparison with Other Models</h2><p>To truly grasp the capabilities of the mT5 model, it&#39;s valuable to briefly explore the achievements of other models in this domain. Let&#39;s delve into a few models supporting at least a few dozen languages for a fair comparison:</p><ol><li><strong>mBERT</strong><br> Starting with mBERT, a multilingual variant of the renowned BERT model, it closely follows BERT&#39;s architecture and objectives. However, the training data diverges, with mBERT relying on text from 104 languages extracted from Wikipedia instead of BERT&#39;s English Wikipedia and Toronto Book Corpus.</li><li><strong>XLM</strong><br> XLM builds upon BERT, incorporating enhanced methods for pre-training multi-lingual language models. It introduces explicitly cross-lingual pre-training objectives to broaden its language capabilities.</li><li><strong>XLM-R</strong><br> As the name suggests, XLM-R is an advancement of the XLM model, based on RoBERTa. Trained on data from Common Crawl in 100 languages, it employs a cross-lingual masked language model objective for improved performance.</li><li><strong>mBART</strong><br> Derived from the BART model, mBART boasts a multilingual encoder-decoder architecture. Its training incorporates a blend of span masking and sentence shuffling objectives. The dataset comprises a subset of 25 languages, sourced from the same data pool as XLM-R.</li><li><strong>MARGE</strong><br> MARGE, a multilingual encoder-decoder model, is trained to reconstruct a document in one language using data extracted from documents in other languages. The dataset encompasses text from 26 diverse languages, originating from Wikipedia and CC-News.</li></ol><p>In order to obtain a quick comparison of the mT5 model with the other multilingual language models mentioned above, check out the table below. It provides a snapshot of key parameters, supported languages, and data sources for models like mBERT, XLM, XLM-R, mBART, MARGE, and finally the mT5</p><table><thead><tr><th>Model</th><th>Architecture</th><th>Parameters</th><th># Languages</th><th>Data Source</th></tr></thead><tbody><tr><td>mBERT</td><td>Encoder-only</td><td>180M</td><td>104</td><td>Wikipedia</td></tr><tr><td>XLM</td><td>Encoder-only</td><td>570M</td><td>100</td><td>Wikipedia</td></tr><tr><td>XLM-R</td><td>Encoder-only</td><td>270M – 550M</td><td>100</td><td>Common Crawl (CCNet)</td></tr><tr><td>mBART</td><td>Encoder-decoder</td><td>680M</td><td>25</td><td>Common Crawl (CC25)</td></tr><tr><td>MARGE</td><td>Encoder-decoder</td><td>960M</td><td>26</td><td>Wikipedia or CC-News</td></tr><tr><td>mT5</td><td>Encoder-decoder</td><td>300M – 13B</td><td>101</td><td>Common Crawl (mC4)</td></tr></tbody></table><p><em>Table: A comparison of multilingual language models.</em></p><h2 id="results" tabindex="-1"><a class="header-anchor" href="#results" aria-hidden="true">#</a> Results</h2><p>Their comprehensive results showcase the exceptional performance of the mT5-XXL model, surpassing state-of-the-art benchmarks in classification and QA tasks, with a nearly competitive standing in NER (69.2 compared to 70.1). Notably, InfoXLM and VECO benefit from parallel training data, while X-STILTs utilizes labeled data akin to the target task. These outcomes emphasize the significance of model capacity in cross-lingual representation learning, suggesting that amplifying a straightforward pre-training approach can be a promising alternative to more intricate methodologies relying on LM filtering, parallel data, or intermediary tasks. Particularly in the &quot;translate-train&quot; scenario, the performance outstrips state-of-the-art standards across all XTREME classification and QA tasks by fine-tuning on a combination of labeled English data and its machine translations.</p><p>In exploring mT5&#39;s performance, a noteworthy trend surfaced across different model sizes and training approaches. Smaller models excelled when trained on gold datasets in various languages compared to weakly supervised or English-only training. However, this advantage diminished for larger models, where the impact of machine translations decreased as model capacity increased. This suggests the potential to bypass costly multilingual data annotation, particularly with larger models. While initially underperforming, larger mT5 models approach dedicated language-specific models&#39; performance, indicating a capacity threshold where the model effectively manages multiple languages without significant interference effects. This observation was reinforced by comparisons between mT5 and T5 models, showcasing that larger mT5 models narrow the performance gap, hinting at a point where the model efficiently learns and handles diverse languages.</p><p align="center"><img src="/assets/results_table.5e51ef1d.png" alt="Results Table"><p align="center"><b>Results of mT5, referenced from the paper</b></p></p><h2 id="zero-shot-generation-setting" tabindex="-1"><a class="header-anchor" href="#zero-shot-generation-setting" aria-hidden="true">#</a> Zero-Shot Generation Setting</h2><p>mT5, a generative model, differs from &quot;encoder-only&quot; models like mBERT by freely generating text predictions. However, in unseen languages during fine-tuning, it occasionally struggles to create well-formed predictions, particularly evident in tasks like XQuAD zero-shot due to inadvertent translation into the fine-tuning language (English). The blog section explores this behavior and its remedy: incorporating a fraction of the multilingual pre-training task during fine-tuning to alleviate these errors.</p><p>In the realm of span selection tasks, the use of generative models, such as mT5, aims to generate &quot;legal&quot; spans that are subsets of the provided context, unlike strict constraints in encoder-based models like BERT. While mT5 consistently produces legal spans on SQuAD, zero-shot cross-lingual span selection poses a more intricate challenge. Despite achieving state-of-the-art results on zero-shot variants of XQuAD, MLQA, and TyDi QA, mT5 encounters issues with illegal predictions. These problematic predictions fall into three main categories: normalization, grammatical adjustments, and accidental translation. For instance, in cases of normalization, certain Unicode characters substitution occurs, notably in Thai, Chinese, and Hindi, which can be rectified through Unicode NFKC normalization. Grammatical adjustments often stem from languages with extensive grammatical case marking, such as Russian, Turkish, and German, where the model modifies the original text for improved grammar. Accidental translation emerges when the model translates part or all of a contextual span into English, despite solely being fine-tuned on English data, leading to partial or full translations into English even within non-English target languages like Greek and Russian. While this spontaneous translation showcases the model&#39;s capabilities, addressing and controlling this behavior remains a challenge for practitioners.</p><p>In addressing the challenge of accidental translation in span selection tasks, the authors explore a more general solution that aligns with the text-to-text framework applicable to all zero-shot generation tasks. Rather than task-specific modifications that limit the model&#39;s predictions to legal spans or outputs, they delve into the model&#39;s learning process during fine-tuning. Recognizing that the absence of exposure to non-English targets during fine-tuning leads the model to favor English outputs, they introduce a strategy inspired by domain-adaptive pre-training, infusing unsupervised multilingual tasks during fine-tuning to maintain the model&#39;s proficiency in generating multiple languages. By incorporating a small portion of multilingual data into the fine-tuning process, they observed a substantial reduction in illegal prediction rates, particularly benefiting mT5-Small and mT5-Base models, ultimately mitigating errors in span selection tasks like XQuAD.</p><h2 id="capabilities-through-examples" tabindex="-1"><a class="header-anchor" href="#capabilities-through-examples" aria-hidden="true">#</a> Capabilities through Examples</h2><p>Now that we have a good understanding of the mT5 model, it makes sense to explore a few examples to grasp its capabilities. As explained earlier, the mT5 model is equipped with the ability to perform various tasks, including summarization, question-answering, and translation.</p><ol><li><p><strong>Summarization of Spanish Text</strong></p><p><em><strong>Input Text:</strong></em></p><pre><code> Tal vez en Italia se inventó la pizza, pero no fue hasta que los napolitanos cruzaron el Atlántico que esta cocina con queso comenzó a despegar.
 La primera pizzería documentada, Lombardi&#39;s, abrió en Manhattan en 1905 y todavía está abierta en la actualidad.
 Antes de eso, había muchos vendedores sin licencia que vendían rebanadas a italoamericanos hambrientos.
 Después de la Segunda Guerra Mundial, el mundo comenzó a clamar por todo lo americano, enviando la pizza a la estratosfera.
 Chefs de todo el mundo comenzaron a experimentar con esta creación centenaria, renovándola con sabores e ingredientes locales como
 la pizza al estilo siciliano. Ya no relegada como una comida no apta, la pizza se convirtió instantáneamente en un clásico,
 lanzando numerosas cadenas e infinitas formas de satisfacer los antojos..  
</code></pre><p><em><strong>Summarized Text:</strong></em></p><pre><code> La pizza se convirtió instantáneamente en un clásico, lanzando numerosas cadenas e infinitas formas de satisfacer los antojos.  
</code></pre></li><li><p><strong>Question-Answering on Turkish Text</strong></p><p><em><strong>Input Text:</strong></em></p><pre><code> Ateş dikkatli bir şekilde ele alınırsa çok faydalı olabilir. Ateş yakabilmek insanlar için her zaman çok önemli olmuştur. İnsanlar soğuk günlerde ısınmak için onun sıcaklığına ihtiyaç duyarlar. Ayrıca et pişirmek için de kullanılır. Işığı insanların karanlık yerleri görmesine ve yırtıcı hayvanları korkutmasına yardımcı oldu.  
</code></pre><p><em><strong>Generated Question Answer Pairs:</strong></em></p><pre><code> Q1. Ateş neye faydalı olur?  
 A1. dikkatli bir şekilde ele alınırsa  

 Q2. İnsanlar ne için sıcaklığına ihtiyaç duymuştur?  
 A2. soğuk günlerde ısınmak için  

 Q3. Ateş hangi amaçla kullanılır?  
 A3. et pişirmek için  

 Q4. Işığı neye yardımcı olmuştur?  
 A4. insanların karanlık yerleri görmesine ve yırtıcı hayvanları korkutmasına  
</code></pre></li><li><p><strong>Translation of Persian Text to English:</strong><br><em><strong>Input Text:</strong></em></p><pre><code> آتش اگر با دقت برخورد شود می تواند بسیار مفید باش
</code></pre><p><em><strong>Translated Text:</strong></em></p><pre><code> The fire might be very helpful if it strikes carefully.  
</code></pre></li><li><p><strong>Summarization of Thai Text:</strong><br><em><strong>Input Text:</strong></em></p><pre><code> ถ้าพูดถึงขนมหวานในตำนานที่ชื่นใจที่สุดแล้วละก็ต้องไม่พ้น น้ำแข็งใส แน่เพราะว่าเป็นอะไรที่ชื่นใจสุด  
</code></pre><p><em><strong>Translated Text:</strong></em></p><pre><code> น้ําแข็งใสเป็นอะไรที่ชื่นใจที่สุด
</code></pre></li></ol><h2 id="wrapping-it-up" tabindex="-1"><a class="header-anchor" href="#wrapping-it-up" aria-hidden="true">#</a> Wrapping it up</h2><p>In conclusion, the mT5 model stands out as a powerful and versatile multilingual variant of the T5 model, demonstrating remarkable capabilities across a diverse range of languages. Its extensive pre-training on the mC4 dataset, encompassing 101 languages, and the innovative zero-sum game approach for balancing linguistic diversity during training contribute to its adaptability. The architecture of mT5, closely following T5 but with improvements, showcases the effectiveness of the encoder-decoder setup. The experiments and results presented in this blog highlight the model&#39;s superior performance, especially in zero-shot and translate-train scenarios, outperforming state-of-the-art benchmarks. Additionally, addressing challenges like accidental translation in span selection tasks underscores the model&#39;s complexity and the ongoing efforts to enhance its practical applications. As we navigate the evolving landscape of multilingual natural language processing, the mT5 model emerges as a promising solution, bridging language gaps and making advanced language processing more accessible to a global audience.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><ol><li>Xue, Linting, et al. &quot;mT5: A massively multilingual pre-trained text-to-text transformer.&quot; arXiv preprint arXiv:2010.11934 (2020).</li><li>Roberts, Adam, et al. &quot;Exploring the limits of transfer learning with a unified text-to-text transformer.&quot; (2019).</li><li>Devlin, Jacob et al. &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.&quot; North American Chapter of the Association for Computational Linguistics (2019).</li><li>Kudo, Taku and John Richardson. &quot;SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing.&quot; Conference on Empirical Methods in Natural Language Processing (2018).</li><li>Lample, Guillaume and Alexis Conneau. &quot;Cross-lingual Language Model Pretraining.&quot; ArXiv abs/1901.07291 (2019): n. pag.</li><li>https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html</li><li>https://www.analyticssteps.com/blogs/what-mt5-google-ai-open-source-multilingual-model-trained-over-101-languages</li><li>https://github.com/google-research/multilingual-t5</li></ol></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/sherlock-jerry/11737-HW3-Blog/edit/main/mt5/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item update-time"><span class="label">Last update: </span><span class="info">11/16/2023, 9:41:14 PM</span></div><div class="meta-item contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: akhil.e2k@gmail.com">Akhil E</span>,<!--]--><!--[--><span class="contributor" title="email: rraghav5600@gmail.com">Sherlock-Jerry</span>,<!--]--><!--[--><span class="contributor" title="email: 42675875+akhil-eppa@users.noreply.github.com">Akhil E</span><!--]--><!--]--></div></footer><!----><div class="giscus-wrapper input-top" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer">Li Lab</div><div class="copyright">Copyright © 2023 Lei Li</div></footer><!--]--></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app.5ae08124.js" defer></script>
  </body>
</html>
